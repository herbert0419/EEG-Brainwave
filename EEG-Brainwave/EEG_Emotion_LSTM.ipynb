{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b     label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00  NEGATIVE  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57   NEUTRAL  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00  POSITIVE  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40  POSITIVE  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60   NEUTRAL  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/home/leo0419/Desktop/emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2132 entries, 0 to 2131\n",
      "Columns: 2549 entries, # mean_0_a to label\n",
      "dtypes: float64(2548), object(1)\n",
      "memory usage: 41.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff720cf6e10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# mean_0_a    0\n",
       "mean_1_a      0\n",
       "mean_2_a      0\n",
       "mean_3_a      0\n",
       "mean_4_a      0\n",
       "             ..\n",
       "fft_746_b     0\n",
       "fft_747_b     0\n",
       "fft_748_b     0\n",
       "fft_749_b     0\n",
       "label         0\n",
       "Length: 2549, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
      "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
      "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
      "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
      "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
      "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
      "\n",
      "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
      "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
      "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
      "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
      "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
      "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
      "\n",
      "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
      "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
      "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
      "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
      "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
      "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
      "\n",
      "[5 rows x 2549 columns]\n",
      "0    716\n",
      "1    708\n",
      "2    708\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "encode = ({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': 2} )\n",
    "#new dataset with replaced values\n",
    "df_encoded = df.replace(encode)\n",
    "\n",
    "print(df_encoded.head())\n",
    "print(df_encoded['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.62</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.70</td>\n",
       "      <td>2.06</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>23.5</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.80</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.88</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-21.8</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.90</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.20</td>\n",
       "      <td>89.90</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>-233.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.90</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.82</td>\n",
       "      <td>2.30</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>-243.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.30</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.3</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.06</td>\n",
       "      <td>41.40</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>38.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  mean_d_1_a  \\\n",
       "0        4.62      30.3    -356.0      15.6      26.3       1.070       0.411   \n",
       "1       28.80      33.1      32.0      25.8      22.8       6.550       1.680   \n",
       "2        8.90      29.4    -416.0      16.7      23.7      79.900       3.360   \n",
       "3       14.90      31.6    -143.0      19.8      24.3      -0.584      -0.284   \n",
       "4       28.30      31.3      45.2      27.3      24.5      34.800      -5.790   \n",
       "\n",
       "   mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  fft_742_b  fft_743_b  \\\n",
       "0      -15.70        2.06        3.15  ...       23.5       20.3       20.3   \n",
       "1        2.88        3.83       -4.82  ...      -23.3      -21.8      -21.8   \n",
       "2       90.20       89.90        2.03  ...      462.0     -233.0     -233.0   \n",
       "3        8.82        2.30       -1.97  ...      299.0     -243.0     -243.0   \n",
       "4        3.06       41.40        5.52  ...       12.0       38.1       38.1   \n",
       "\n",
       "   fft_744_b  fft_745_b  fft_746_b  fft_747_b  fft_748_b  fft_749_b  label  \n",
       "0       23.5     -215.0     280.00    -162.00    -162.00     280.00      2  \n",
       "1      -23.3      182.0       2.57     -31.60     -31.60       2.57      0  \n",
       "2      462.0     -267.0     281.00    -148.00    -148.00     281.00      1  \n",
       "3      299.0      132.0     -12.40       9.53       9.53     -12.40      1  \n",
       "4       12.0      119.0     -17.60      23.90      23.90     -17.60      0  \n",
       "\n",
       "[5 rows x 2549 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132, 2548)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df_encoded.drop([\"label\"]  ,axis=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2132,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df_encoded.loc[:,'label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0],1,x.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],1,x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(1,2548),activation=\"relu\",return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(32,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 64)             668928    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 681,443\n",
      "Trainable params: 681,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.7257 - accuracy: 0.7971 - val_loss: 0.5095 - val_accuracy: 0.9110\n",
      "Epoch 2/100\n",
      "54/54 [==============================] - 1s 28ms/step - loss: 0.4464 - accuracy: 0.9314 - val_loss: 0.3896 - val_accuracy: 0.9180\n",
      "Epoch 3/100\n",
      "54/54 [==============================] - 1s 25ms/step - loss: 0.3366 - accuracy: 0.9413 - val_loss: 0.3178 - val_accuracy: 0.9157\n",
      "Epoch 4/100\n",
      "54/54 [==============================] - 1s 25ms/step - loss: 0.2817 - accuracy: 0.9443 - val_loss: 0.2876 - val_accuracy: 0.9227\n",
      "Epoch 5/100\n",
      "54/54 [==============================] - 1s 24ms/step - loss: 0.2381 - accuracy: 0.9507 - val_loss: 0.2509 - val_accuracy: 0.9251\n",
      "Epoch 6/100\n",
      "54/54 [==============================] - 2s 28ms/step - loss: 0.2053 - accuracy: 0.9513 - val_loss: 0.2105 - val_accuracy: 0.9297\n",
      "Epoch 7/100\n",
      "54/54 [==============================] - 1s 24ms/step - loss: 0.1788 - accuracy: 0.9566 - val_loss: 0.1879 - val_accuracy: 0.9344\n",
      "Epoch 8/100\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 0.1543 - accuracy: 0.9619 - val_loss: 0.1746 - val_accuracy: 0.9391\n",
      "Epoch 9/100\n",
      "54/54 [==============================] - 1s 26ms/step - loss: 0.1214 - accuracy: 0.9730 - val_loss: 0.1558 - val_accuracy: 0.9461\n",
      "Epoch 10/100\n",
      "54/54 [==============================] - 1s 28ms/step - loss: 0.1000 - accuracy: 0.9801 - val_loss: 0.1373 - val_accuracy: 0.9578\n",
      "Epoch 11/100\n",
      "54/54 [==============================] - 2s 28ms/step - loss: 0.0734 - accuracy: 0.9900 - val_loss: 0.1369 - val_accuracy: 0.9578\n",
      "Epoch 12/100\n",
      "54/54 [==============================] - 2s 38ms/step - loss: 0.0748 - accuracy: 0.9848 - val_loss: 0.1589 - val_accuracy: 0.9578\n",
      "Epoch 13/100\n",
      "54/54 [==============================] - 2s 30ms/step - loss: 0.0690 - accuracy: 0.9865 - val_loss: 0.1304 - val_accuracy: 0.9649\n",
      "Epoch 14/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0521 - accuracy: 0.9930 - val_loss: 0.1223 - val_accuracy: 0.9625\n",
      "Epoch 15/100\n",
      "54/54 [==============================] - 2s 36ms/step - loss: 0.0492 - accuracy: 0.9912 - val_loss: 0.1338 - val_accuracy: 0.9696\n",
      "Epoch 16/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0338 - accuracy: 0.9965 - val_loss: 0.0976 - val_accuracy: 0.9719\n",
      "Epoch 17/100\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 0.0363 - accuracy: 0.9953 - val_loss: 0.1255 - val_accuracy: 0.9672\n",
      "Epoch 18/100\n",
      "54/54 [==============================] - 1s 24ms/step - loss: 0.0295 - accuracy: 0.9971 - val_loss: 0.1290 - val_accuracy: 0.9672\n",
      "Epoch 19/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0302 - accuracy: 0.9959 - val_loss: 0.1033 - val_accuracy: 0.9719\n",
      "Epoch 20/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0248 - accuracy: 0.9982 - val_loss: 0.1114 - val_accuracy: 0.9696\n",
      "Epoch 21/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0309 - accuracy: 0.9941 - val_loss: 0.1358 - val_accuracy: 0.9649\n",
      "Epoch 22/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.0228 - accuracy: 0.9982 - val_loss: 0.1578 - val_accuracy: 0.9555\n",
      "Epoch 23/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.0315 - accuracy: 0.9935 - val_loss: 0.1352 - val_accuracy: 0.9602\n",
      "Epoch 24/100\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 0.0187 - accuracy: 0.9982 - val_loss: 0.1142 - val_accuracy: 0.9672\n",
      "Epoch 25/100\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.0195 - accuracy: 0.9965 - val_loss: 0.1877 - val_accuracy: 0.9485\n",
      "Epoch 26/100\n",
      "54/54 [==============================] - 2s 30ms/step - loss: 0.0264 - accuracy: 0.9965 - val_loss: 0.1300 - val_accuracy: 0.9649\n",
      "Epoch 27/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0159 - accuracy: 0.9988 - val_loss: 0.1470 - val_accuracy: 0.9625\n",
      "Epoch 28/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9649\n",
      "Epoch 29/100\n",
      "54/54 [==============================] - 2s 28ms/step - loss: 0.0276 - accuracy: 0.9924 - val_loss: 0.1098 - val_accuracy: 0.9649\n",
      "Epoch 30/100\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 0.0342 - accuracy: 0.9935 - val_loss: 0.1437 - val_accuracy: 0.9578\n",
      "Epoch 31/100\n",
      "54/54 [==============================] - 1s 25ms/step - loss: 0.0181 - accuracy: 0.9965 - val_loss: 0.1277 - val_accuracy: 0.9649\n",
      "Epoch 32/100\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 0.0224 - accuracy: 0.9935 - val_loss: 0.1453 - val_accuracy: 0.9625\n",
      "Epoch 33/100\n",
      "54/54 [==============================] - 2s 36ms/step - loss: 0.0147 - accuracy: 0.9988 - val_loss: 0.1271 - val_accuracy: 0.9696\n",
      "Epoch 34/100\n",
      "54/54 [==============================] - 2s 29ms/step - loss: 0.0284 - accuracy: 0.9924 - val_loss: 0.1582 - val_accuracy: 0.9672\n",
      "Epoch 35/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.0229 - accuracy: 0.9935 - val_loss: 0.1153 - val_accuracy: 0.9649\n",
      "Epoch 36/100\n",
      "54/54 [==============================] - 2s 37ms/step - loss: 0.0193 - accuracy: 0.9965 - val_loss: 0.1565 - val_accuracy: 0.9602\n",
      "Epoch 37/100\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.0299 - accuracy: 0.9912 - val_loss: 0.2406 - val_accuracy: 0.9415\n",
      "Epoch 38/100\n",
      "54/54 [==============================] - 2s 38ms/step - loss: 0.0224 - accuracy: 0.9941 - val_loss: 0.1638 - val_accuracy: 0.9555\n",
      "Epoch 39/100\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0185 - accuracy: 0.9959 - val_loss: 0.1569 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.0154 - accuracy: 0.9965 - val_loss: 0.1285 - val_accuracy: 0.9696\n",
      "Epoch 41/100\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.0271 - accuracy: 0.9935 - val_loss: 0.1324 - val_accuracy: 0.9672\n",
      "Epoch 42/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0153 - accuracy: 0.9953 - val_loss: 0.0963 - val_accuracy: 0.9766\n",
      "Epoch 43/100\n",
      "54/54 [==============================] - 2s 34ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.1456 - val_accuracy: 0.9649\n",
      "Epoch 44/100\n",
      "54/54 [==============================] - 2s 30ms/step - loss: 0.0098 - accuracy: 0.9988 - val_loss: 0.1555 - val_accuracy: 0.9555\n",
      "Epoch 45/100\n",
      "54/54 [==============================] - 1s 23ms/step - loss: 0.0086 - accuracy: 0.9994 - val_loss: 0.1787 - val_accuracy: 0.9555\n",
      "Epoch 46/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0088 - accuracy: 0.9988 - val_loss: 0.1478 - val_accuracy: 0.9625\n",
      "Epoch 47/100\n",
      "54/54 [==============================] - 2s 33ms/step - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.1444 - val_accuracy: 0.9672\n",
      "Epoch 48/100\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 0.1768 - val_accuracy: 0.9649\n",
      "Epoch 49/100\n",
      "54/54 [==============================] - 2s 32ms/step - loss: 0.0149 - accuracy: 0.9965 - val_loss: 0.1572 - val_accuracy: 0.9625\n",
      "Epoch 50/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.1816 - val_accuracy: 0.9672\n",
      "Epoch 51/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.0120 - accuracy: 0.9977 - val_loss: 0.1213 - val_accuracy: 0.9719\n",
      "Epoch 52/100\n",
      "54/54 [==============================] - 2s 31ms/step - loss: 0.0085 - accuracy: 0.9977 - val_loss: 0.1540 - val_accuracy: 0.9602\n",
      "Epoch 53/100\n",
      "54/54 [==============================] - 1s 26ms/step - loss: 0.0076 - accuracy: 0.9994 - val_loss: 0.1472 - val_accuracy: 0.9625\n",
      "Epoch 54/100\n",
      "54/54 [==============================] - 2s 29ms/step - loss: 0.0191 - accuracy: 0.9947 - val_loss: 0.1489 - val_accuracy: 0.9602\n",
      "Epoch 55/100\n",
      "54/54 [==============================] - 2s 30ms/step - loss: 0.0168 - accuracy: 0.9977 - val_loss: 0.2106 - val_accuracy: 0.9532\n",
      "Epoch 56/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 0.1678 - val_accuracy: 0.9578\n",
      "Epoch 57/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.1530 - val_accuracy: 0.9602\n",
      "Epoch 58/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0181 - accuracy: 0.9947 - val_loss: 0.1369 - val_accuracy: 0.9696\n",
      "Epoch 59/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.1850 - val_accuracy: 0.9649\n",
      "Epoch 60/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0057 - accuracy: 0.9994 - val_loss: 0.1570 - val_accuracy: 0.9696\n",
      "Epoch 61/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.0124 - accuracy: 0.9977 - val_loss: 0.1111 - val_accuracy: 0.9789\n",
      "Epoch 62/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.1208 - val_accuracy: 0.9742\n",
      "Epoch 63/100\n",
      "54/54 [==============================] - 2s 28ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 0.1075 - val_accuracy: 0.9813\n",
      "Epoch 64/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.0988 - val_accuracy: 0.9813\n",
      "Epoch 65/100\n",
      "54/54 [==============================] - 1s 19ms/step - loss: 0.0106 - accuracy: 0.9959 - val_loss: 0.1394 - val_accuracy: 0.9672\n",
      "Epoch 66/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.1156 - val_accuracy: 0.9789\n",
      "Epoch 67/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.1356 - val_accuracy: 0.9742\n",
      "Epoch 68/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.1432 - val_accuracy: 0.9696\n",
      "Epoch 69/100\n",
      "54/54 [==============================] - 1s 21ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.1100 - val_accuracy: 0.9789\n",
      "Epoch 70/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.1329 - val_accuracy: 0.9696\n",
      "Epoch 71/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.1212 - val_accuracy: 0.9742\n",
      "Epoch 72/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9696\n",
      "Epoch 73/100\n",
      "54/54 [==============================] - 1s 20ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.1356 - val_accuracy: 0.9719\n",
      "Epoch 74/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.1870 - val_accuracy: 0.9578\n",
      "Epoch 75/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.1791 - val_accuracy: 0.9602\n",
      "Epoch 76/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.2064 - val_accuracy: 0.9532\n",
      "Epoch 77/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.2130 - val_accuracy: 0.9578\n",
      "Epoch 78/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.1999 - val_accuracy: 0.9602\n",
      "Epoch 79/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.1649 - val_accuracy: 0.9649\n",
      "Epoch 80/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.1636 - val_accuracy: 0.9649\n",
      "Epoch 81/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2125 - val_accuracy: 0.9578\n",
      "Epoch 82/100\n",
      "54/54 [==============================] - 1s 15ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.1926 - val_accuracy: 0.9625\n",
      "Epoch 83/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.1731 - val_accuracy: 0.9649\n",
      "Epoch 84/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9602\n",
      "Epoch 85/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.2161 - val_accuracy: 0.9602\n",
      "Epoch 86/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2158 - val_accuracy: 0.9602\n",
      "Epoch 87/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.2259 - val_accuracy: 0.9602\n",
      "Epoch 88/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.1989 - val_accuracy: 0.9625\n",
      "Epoch 89/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.1779 - val_accuracy: 0.9672\n",
      "Epoch 90/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1840 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1919 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.1895 - val_accuracy: 0.9649\n",
      "Epoch 93/100\n",
      "54/54 [==============================] - 1s 16ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.1412 - val_accuracy: 0.9742\n",
      "Epoch 94/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.1546 - val_accuracy: 0.9719\n",
      "Epoch 95/100\n",
      "54/54 [==============================] - 1s 17ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.1555 - val_accuracy: 0.9696\n",
      "Epoch 96/100\n",
      "54/54 [==============================] - 1s 18ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.1555 - val_accuracy: 0.9719\n",
      "Epoch 97/100\n",
      "54/54 [==============================] - 1s 26ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.1690 - val_accuracy: 0.9742\n",
      "Epoch 98/100\n",
      "54/54 [==============================] - 1s 27ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.1585 - val_accuracy: 0.9742\n",
      "Epoch 99/100\n",
      "54/54 [==============================] - 1s 28ms/step - loss: 0.0202 - accuracy: 0.9947 - val_loss: 0.1686 - val_accuracy: 0.9672\n",
      "Epoch 100/100\n",
      "54/54 [==============================] - 1s 28ms/step - loss: 0.0127 - accuracy: 0.9965 - val_loss: 0.1827 - val_accuracy: 0.9625\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1827 - accuracy: 0.9625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 100, validation_data= (x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427,)\n",
      "(427,)\n",
      "Training Accuracy: 0.9625292740046838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = model.predict(x_test)\n",
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(expected_classes.shape)\n",
    "print(predict_classes.shape)\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(f\"Training Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report,roc_curve,accuracy_score,auc\n",
    "def plot_confusion_matrix(cm,lables):\n",
    "    fig, ax = plt.subplots(figsize=(12,8)) # for plotting confusion matrix as image\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "    yticks=np.arange(cm.shape[0]),\n",
    "    xticklabels=lables, yticklabels=lables,\n",
    "    ylabel='True label',\n",
    "    xlabel='Predicted label')\n",
    "    plt.xticks(rotation = 90)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, int(cm[i, j]),ha=\"center\", va=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_test)\n",
    "# y_pred = np.argmax(y_pred,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
